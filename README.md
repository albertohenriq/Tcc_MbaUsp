# ğŸš€ ComparaÃ§Ã£o de Performance: REST vs gRPC - TCC

Este projeto implementa um **ambiente completo de testes** para comparaÃ§Ã£o empÃ­rica de desempenho entre protocolos **REST (HTTP/JSON)** e **gRPC (HTTP/2/Protobuf)** em uma arquitetura de microserviÃ§os.

## ğŸ¯ Objetivos do Projeto

- **AnÃ¡lise Quantitativa**: ComparaÃ§Ã£o de latÃªncia, throughput e recursos
- **AnÃ¡lise Qualitativa**: Facilidade de implementaÃ§Ã£o, manutenibilidade e curva de aprendizado
- **Testes PrÃ¡ticos**: CenÃ¡rios reais de escalabilidade e resiliÃªncia
- **RecomendaÃ§Ãµes**: Guidelines para escolha de protocolo baseado em dados empÃ­ricos

## ğŸ—ï¸ Arquitetura Implementada

### Infraestrutura
- **Hardware**: Dell i7, 16GB RAM, SSD
- **Plataforma**: Docker Desktop + WSL2
- **Monitoramento**: Prometheus + Grafana + cAdvisor

### MicroserviÃ§os
- **ServiÃ§o A (Node.js)**: Gateway/API Layer - Suporte REST + gRPC
- **ServiÃ§o B (Python)**: Processing Service - LÃ³gica de negÃ³cio
- **ServiÃ§o C (Node.js)**: Storage/Backend - PersistÃªncia de dados

### Stack TecnolÃ³gica
- **ContainerizaÃ§Ã£o**: Docker + Docker Compose
- **Monitoramento**: Prometheus + Grafana + cAdvisor
- **Testes**: k6 (Load Testing) + Scripts de AnÃ¡lise Python
- **Protocolos**: REST (Express.js) + gRPC (Protocol Buffers)
- **Observabilidade**: Dashboards tempo real + AnÃ¡lise de logs

## ğŸ“ Estrutura do Projeto

```
FinalTcc/
â”œâ”€â”€ ğŸ“¦ src/                                    # CÃ³digo fonte dos microserviÃ§os
â”‚   â”œâ”€â”€ service-a-nodejs/                      # Gateway (REST + gRPC)
â”‚   â”‚   â”œâ”€â”€ src/index.js
â”‚   â”‚   â”œâ”€â”€ package.json
â”‚   â”‚   â””â”€â”€ proto/processing.proto
â”‚   â”œâ”€â”€ service-b-python/                      # Processing Service
â”‚   â”‚   â”œâ”€â”€ app/main.py
â”‚   â”‚   â”œâ”€â”€ app/grpc_server.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ proto/processing.proto
â”‚   â””â”€â”€ service-c-nodejs/                      # Storage Service
â”‚       â”œâ”€â”€ src/index.js
â”‚       â””â”€â”€ package.json
â”œâ”€â”€ ğŸ³ docker/                                 # ConfiguraÃ§Ãµes de monitoramento
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ dashboards/                        # Dashboards prÃ©-configurados
â”‚   â”‚   â””â”€â”€ datasources/
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml                     # ConfiguraÃ§Ã£o de mÃ©tricas
â”œâ”€â”€ ğŸ§ª k6-tests/                               # Scripts de teste de carga
â”‚   â”œâ”€â”€ monitoring-collection-test.js          # Teste principal de monitoramento
â”‚   â”œâ”€â”€ comparison/                            # Testes comparativos
â”‚   â”œâ”€â”€ final/                                 # Testes finais validados
â”‚   â””â”€â”€ *.js                                   # Diversos cenÃ¡rios de teste
â”œâ”€â”€ ğŸ“Š docs/                                   # DocumentaÃ§Ã£o tÃ©cnica
â”‚   â”œâ”€â”€ arquitetura_servicos.md
â”‚   â”œâ”€â”€ exemplos_uso.md
â”‚   â””â”€â”€ experimento/                           # Planos e resultados
â”œâ”€â”€ ğŸš€ Scripts de ExecuÃ§Ã£o                     # AutomaÃ§Ã£o de testes
â”‚   â”œâ”€â”€ run_simple_monitoring.ps1             # Monitoramento completo
â”‚   â”œâ”€â”€ run_scalability_tests.ps1             # Testes de escalabilidade
â”‚   â”œâ”€â”€ run_resilience_tests.ps1              # Testes de resiliÃªncia
â”‚   â””â”€â”€ preparar_github.ps1                   # Setup para GitHub
â”œâ”€â”€ ğŸ” Scripts de AnÃ¡lise                      # Processamento de dados
â”‚   â”œâ”€â”€ analyze_k6_logs_fixed.py              # AnÃ¡lise de logs k6
â”‚   â”œâ”€â”€ analise_forma_comparativa.py          # AnÃ¡lise comparativa
â”‚   â””â”€â”€ gerar_tabelas_executivas.py           # GeraÃ§Ã£o de tabelas
â”œâ”€â”€ ğŸ“‹ RelatÃ³rios e DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ 5_FORMA_DE_ANALISE_COMPLETA.md        # AnÃ¡lise comparativa final
â”‚   â”œâ”€â”€ RELATORIO_COLETA_DADOS_COMPLETO.md    # RelatÃ³rio de coleta
â”‚   â”œâ”€â”€ RESUMO_EXECUTIVO_5_FORMA_ANALISE.md   # Resumo executivo
â”‚   â””â”€â”€ *.csv                                  # Tabelas de dados
â”œâ”€â”€ docker-compose.yml                         # OrquestraÃ§Ã£o completa
â”œâ”€â”€ .gitignore                                 # ConfiguraÃ§Ã£o Git
â””â”€â”€ README.md                                  # Este arquivo
```

## ğŸš€ Como Executar os Testes

### PrÃ©-requisitos
- **Docker Desktop** instalado e funcionando
- **Node.js** v16+ (para desenvolvimento local)
- **Python** 3.8+ (para scripts de anÃ¡lise)
- **k6** para testes de carga
- **PowerShell** (Windows) ou **Bash** (Linux/Mac)

### 1. Setup Inicial
```powershell
# Clone o repositÃ³rio
git clone https://github.com/SEU_USUARIO/final-tcc.git
cd final-tcc

# Inicie o ambiente completo
docker-compose up -d
```

### 2. ExecuÃ§Ã£o Automatizada (Recomendado)
```powershell
# Teste completo de monitoramento (26 min)
.\run_simple_monitoring.ps1

# Testes de escalabilidade (4 configuraÃ§Ãµes)
.\run_scalability_tests.ps1

# Testes de resiliÃªncia
.\run_resilience_tests.ps1
```

### 3. ExecuÃ§Ã£o Manual
```powershell
# Teste REST vs gRPC bÃ¡sico
k6 run k6-tests/monitoring-collection-test.js

# Teste de escalabilidade especÃ­fico
k6 run k6-tests/comparison/rest-vs-grpc-500.js

# Teste de resiliÃªncia
k6 run k6-tests/resilience-rest-test.js
```

### 4. AnÃ¡lise de Resultados
```powershell
# AnÃ¡lise automÃ¡tica de logs k6
python analyze_k6_logs_fixed.py

# GeraÃ§Ã£o de anÃ¡lise comparativa
python analise_forma_comparativa.py

# CriaÃ§Ã£o de tabelas executivas
python gerar_tabelas_executivas.py
```

### 5. Acesso aos Dashboards
- **Grafana**: http://localhost:3010 (admin/admin)
- **Prometheus**: http://localhost:9090
- **cAdvisor**: http://localhost:8080
- **Service A**: http://localhost:3000

## ğŸ§ª CenÃ¡rios de Teste Implementados

### 1. ğŸ“Š Performance Comparativa
- **Cargas**: 100, 300, 500, 1000 usuÃ¡rios virtuais
- **DuraÃ§Ã£o**: 26 minutos (teste completo de monitoramento)
- **Ramp-up**: 5 estÃ¡gios de crescimento gradual
- **Payload**: Estrutura JSON/Protobuf com dados de processamento
- **Protocolos**: REST (HTTP/JSON) vs gRPC (HTTP/2/Protobuf)

### 2. ğŸ”„ Escalabilidade Horizontal
- **ConfiguraÃ§Ãµes**: 1, 2, 4, 8 rÃ©plicas do ServiÃ§o B
- **Carga**: 500 VUs por 10 minutos
- **MÃ©tricas**: Throughput, latÃªncia P95/P99, utilizaÃ§Ã£o de recursos
- **Objetivo**: Avaliar eficÃ¡cia da escalabilidade horizontal

### 3. ğŸ›¡ï¸ ResiliÃªncia e RecuperaÃ§Ã£o
- **CenÃ¡rio**: Falhas simuladas no ServiÃ§o B
- **Circuit Breaker**: Implementado e testado
- **MÃ©tricas**: Tempo de recuperaÃ§Ã£o, degradaÃ§Ã£o graceful
- **Patterns**: Timeout, retry, fallback

### 4. ğŸ“ˆ Monitoramento ContÃ­nuo
- **DuraÃ§Ã£o**: Testes de 26+ minutos
- **MÃ©tricas**: CPU, memÃ³ria, network I/O
- **Ferramentas**: Prometheus + Grafana + cAdvisor
- **FrequÃªncia**: Coleta a cada 15 segundos

## Coleta de Dados

### MÃ©tricas Coletadas
- Tempo mÃ©dio de resposta
- LatÃªncia (p95, p99)
- Throughput (req/s)
- Taxa de erros
- Uso de CPU/memÃ³ria
- Tempo de recuperaÃ§Ã£o apÃ³s falhas

### VisualizaÃ§Ã£o
- Dashboards Grafana
- RelatÃ³rios k6/JMeter
- AnÃ¡lise comparativa em grÃ¡ficos

## Resultados Esperados

A anÃ¡lise final incluirÃ¡:
1. ComparaÃ§Ã£o quantitativa de desempenho
2. AnÃ¡lise de escalabilidade
3. AvaliaÃ§Ã£o de resiliÃªncia
4. ConsideraÃ§Ãµes sobre implementaÃ§Ã£o e manutenÃ§Ã£o
5. RecomendaÃ§Ãµes prÃ¡ticas de uso
